import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler
import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, utils
from tensorflow.keras.utils import to_categorical
from sklearn.metrics import classification_report
from sklearn.preprocessing import StandardScaler, MinMaxScaler
import pickle
from matplotlib import pyplot as plt
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression, SGDClassifier, RidgeClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, roc_curve, auc
import itertools
import random

# Caricamento del dataset pre-processato
tr = pd.read_csv("matches_final.csv", index_col=False)

# Separazione delle feature (X) e del target (y)
X = tr.drop('result', axis=1)
y = tr['result']

# Identificazione delle colonne categoriche e numeriche
categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()
numerical_cols = X.select_dtypes(include=['float64', 'int64']).columns.tolist()

# One-Hot Encoding per le variabili categoriche
X_categorical_encoded = pd.get_dummies(X[categorical_cols], columns=categorical_cols)

# Standardizzazione delle variabili numeriche
scaler = StandardScaler()
X_numerical_scaled = pd.DataFrame(scaler.fit_transform(X[numerical_cols]), columns=numerical_cols)

# Unione delle variabili categoriche codificate e numeriche standardizzate
X_categorical_encoded = X_categorical_encoded.reset_index(drop=True)
X_numerical_scaled = X_numerical_scaled.reset_index(drop=True)
X_final = pd.concat([X_categorical_encoded, X_numerical_scaled], axis=1)

# Codifica del target (y) in valori numerici
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)

# Creazione del test set (ultima stagione)
test_x = X_final.tail(761)
test_y = y_encoded[-761:]
test_x['result'] = test_y
test_x.to_csv("test_set.csv", index=False)

# Rimozione dell'ultima stagione dal training set
X_final = X_final.iloc[:-761]
y_encoded = y_encoded[:-761]

# Divisione in training e validation set
X_train, X_val, y_train, y_val = train_test_split(X_final, y_encoded, test_size=0.2, random_state=42)

# Funzione per creare una rete neurale
def create_network(input_dim, neurons_1layer, neurons_2layer, activation_function):
    inputs = tf.keras.Input((input_dim,))
    x = layers.Dense(neurons_1layer, activation_function)(inputs)
    x = layers.Dense(neurons_2layer, activation_function)(x)
    x = layers.Dropout(0.1)(x)
    output = layers.Dense(3, "softmax")(x)
    model = tf.keras.Model(inputs=inputs, outputs=output, name="neural_net")
    return model

# Definizione dello spazio degli iperparametri per Random Search
RANDOM_SEARCH_SPACE = {
    "learning_rate": [1e-3, 1e-4, 1e-5],
    "epochs": [5, 10, 15, 20],
    "neurons_1layer": [30, 50, 70],
    "neurons_2layer": [20, 40, 60],
    "activation_functions": ['relu', 'sigmoid', 'tanh'],
    "batch_size": [100, 200, 300]
}

# Numero di combinazioni casuali da provare
NUM_RANDOM_COMBINATIONS = 20

# Variabili per tenere traccia dei migliori iperparametri
best_params = None
best_val_loss = np.inf

# Ciclo per testare combinazioni casuali di iperparametri
for _ in range(NUM_RANDOM_COMBINATIONS):
    # Selezione casuale degli iperparametri
    learning_rate = random.choice(RANDOM_SEARCH_SPACE["learning_rate"])
    epochs = random.choice(RANDOM_SEARCH_SPACE["epochs"])
    neurons_1layer = random.choice(RANDOM_SEARCH_SPACE["neurons_1layer"])
    neurons_2layer = random.choice(RANDOM_SEARCH_SPACE["neurons_2layer"])
    activation_function = random.choice(RANDOM_SEARCH_SPACE["activation_functions"])
    batch_size = random.choice(RANDOM_SEARCH_SPACE["batch_size"])

    print(f"Testing combination: lr={learning_rate}, epochs={epochs}, neurons_1layer={neurons_1layer}, neurons_2layer={neurons_2layer}, activation={activation_function}, batch_size={batch_size}")

    # Creazione del modello con i parametri correnti
    model = create_network(X_train.shape[1], neurons_1layer, neurons_2layer, activation_function)

    # Compilazione del modello
    model.compile(
        loss='sparse_categorical_crossentropy',
        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),
        metrics=['accuracy']
    )

    # Addestramento del modello
    history = model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=epochs,
        batch_size=batch_size,
        verbose=0  # Nessun output durante la ricerca
    )

    # Valutazione della loss sul validation set
    final_val_loss = history.history['val_loss'][-1]

    print(f"Validation loss: {final_val_loss}")

    # Aggiornamento dei migliori iperparametri
    if final_val_loss < best_val_loss:
        best_val_loss = final_val_loss
        best_params = {
            "learning_rate": learning_rate,
            "epochs": epochs,
            "neurons_1layer": neurons_1layer,
            "neurons_2layer": neurons_2layer,
            "activation_function": activation_function,
            "batch_size": batch_size
        }

# Stampa dei migliori iperparametri
print("Best hyperparameters found:")
print(best_params)
print(f"Best validation loss: {best_val_loss}")

# Creazione del modello con i migliori iperparametri
best_model = create_network(
    X_train.shape[1],
    best_params["neurons_1layer"],
    best_params["neurons_2layer"],
    best_params["activation_function"]
)

# Compilazione del modello
best_model.compile(
    loss='sparse_categorical_crossentropy',
    optimizer=tf.keras.optimizers.Adam(learning_rate=best_params["learning_rate"]),
    metrics=['accuracy']
)

# Addestramento del modello
best_history = best_model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=best_params["epochs"],
    batch_size=best_params["batch_size"],
    verbose=1  # Mostra il progresso
)

# Valutazione del modello sul validation set
final_val_loss = best_history.history['val_loss'][-1]
final_val_accuracy = best_history.history['val_accuracy'][-1]

print(f"Final validation loss with best hyperparameters: {final_val_loss}")
print(f"Final validation accuracy with best hyperparameters: {final_val_accuracy}")

# Plot della loss durante l'addestramento
plt.figure(figsize=(8, 6))
plt.plot(best_history.history['loss'], label='Train Loss')
plt.plot(best_history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
# Salva la figura in un file
plt.savefig('model_loss.png')
plt.show()

# Salvataggio del modello
best_model.save("best_model.h5")